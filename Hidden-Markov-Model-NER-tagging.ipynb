{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import conllu as cl\n",
    "from os.path import join\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(language):\n",
    "    filePath = \".\\Datasets\\\\\" + language + \"\\\\\"\n",
    "    \n",
    "    with open(join(filePath, \"corpus.conllu\"), 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    train_sentences = cl.parse(data)\n",
    "\n",
    "    with open(join(filePath, \"testCorpus.conllu\"), 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    test_sentences = cl.parse(data)\n",
    "\n",
    "    with open(join(filePath, \"val.conllu\"), 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    val_sentences = cl.parse(data)\n",
    "\n",
    "    return train_sentences, test_sentences, val_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the HMM model with the train sentences, it generates 2 outputps containing the probabilities that will be used \n",
    "# in the decoding phase.\n",
    "def HMMFit(train_sentences):\n",
    "    # dictionary of dictionaries\n",
    "    countTag = {}   # Count(tag(i))\n",
    "    transitionCount = {}    # Count(tag(i-1), tag(i))\n",
    "    wordTagCount = {}   # Count(tag(i), word(i))\n",
    "    tagDistribution = {}    # Count(tag(i-1), tag(i)) / Count(tag(i-1))\n",
    "    wordDistribution = {}   # Count(tag(i), word(i))\n",
    "\n",
    "    # complete creation of counTag dictorary\n",
    "    # partial initialization of transitionCount dictionary\n",
    "    # partial creation of wordTagCount dictionary\n",
    "    for sentence in train_sentences:\n",
    "        for word in sentence:\n",
    "            if word['lemma'] in countTag:\n",
    "                countTag[word['lemma']] += 1\n",
    "            else:\n",
    "                countTag[word['lemma']] = 1\n",
    "                transitionCount[word['lemma']] = {}\n",
    "            \n",
    "            if word['form'] not in wordTagCount:\n",
    "                wordTagCount[word['form']] = {}\n",
    "            if word['lemma'] in wordTagCount[word['form']]:\n",
    "                wordTagCount[word['form']][word['lemma']] += 1\n",
    "            else:\n",
    "                wordTagCount[word['form']][word['lemma']] = 1\n",
    "    \n",
    "    # complete inizialization of transitionCount dictionary\n",
    "    for key in transitionCount.keys():\n",
    "        for tag in countTag.keys():\n",
    "            transitionCount[key][tag] = 0\n",
    "\n",
    "    # complete creation of wordTagCount dictionary\n",
    "    for word in wordTagCount.keys():\n",
    "        for tag in countTag.keys():\n",
    "            if tag not in wordTagCount[word]:\n",
    "                wordTagCount[word][tag] = 0\n",
    "\n",
    "    # complete creation of transitionCount dictionary\n",
    "    # previousTag --> Markov's hypothesis of first grade\n",
    "    for tag in transitionCount.keys():\n",
    "        for sentence in train_sentences:\n",
    "            previousTag = None\n",
    "            for word in sentence:\n",
    "                if previousTag == None:\n",
    "                    previousTag = word['lemma']\n",
    "                else:\n",
    "                    if word['lemma'] == tag:\n",
    "                        if previousTag in transitionCount[tag]:\n",
    "                            transitionCount[tag][previousTag] +=1\n",
    "                    previousTag = word['lemma']\n",
    "    \n",
    "    # complete creation of tagDistribution dictionary\n",
    "    # we use the log-probability for better accuracy (since the probability are very small)\n",
    "    # if the probability is 0, then the log-probability will be log(10**-10) \n",
    "    epsilon = 10**-10\n",
    "    for eventTag in countTag.keys():\n",
    "        tagDistribution[eventTag] = {}\n",
    "        for condTag in countTag.keys():\n",
    "            if transitionCount[eventTag][condTag]/countTag[condTag] == 0:\n",
    "                tagDistribution[eventTag][condTag] = math.log(epsilon)\n",
    "            else:\n",
    "                tagDistribution[eventTag][condTag] = math.log(transitionCount[eventTag][condTag]/countTag[condTag])\n",
    "            \n",
    "    # complete creation of wordDistribution dictionary\n",
    "    # we use the log-probability for better accuracy (since the probability are very small)\n",
    "    # if the probability is 0, then the log-probability will be log(10**-10)\n",
    "    for word in wordTagCount.keys():\n",
    "        wordDistribution[word] = {}\n",
    "        for tag in countTag.keys():\n",
    "            if wordTagCount[word][tag]/countTag[tag] == 0:\n",
    "                wordDistribution[word][tag] = math.log(epsilon)\n",
    "            else:\n",
    "                wordDistribution[word][tag] = math.log(wordTagCount[word][tag]/countTag[tag])\n",
    "        \n",
    "    return tagDistribution, wordDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbiAlgorithm(sentence, tagDistribution, wordDistribution, b_value):\n",
    "    viterbi = {}\n",
    "    backpointer = {}\n",
    "\n",
    "    # todo: add the descrpt.\n",
    "    for word in sentence:\n",
    "        viterbi[word['form']] = {}\n",
    "        backpointer[word['form']] = {}\n",
    "\n",
    "    # initialization step\n",
    "    for tag in tagDistribution.keys():\n",
    "        viterbi[(sentence[0])['form']][tag] = wordDistribution[(sentence[0])['form']][tag] * b_value\n",
    "        backpointer[(sentence[0])['form']][tag] = 0\n",
    "\n",
    "    # recursion step\n",
    "    print(len(sentence))\n",
    "    print(sentence)\n",
    "    for i in range(1, len(sentence)):\n",
    "        print((sentence[i])['form'])\n",
    "        for tag in tagDistribution.keys():\n",
    "            viterbi[(sentence[i])['form']][tag] = max(viterbi[(sentence[i-1])['form']][tagLoop] + wordDistribution[(sentence[i])['form']][tagLoop] + tagDistribution[tag][tagLoop] for tagLoop in tagDistribution.keys())\n",
    "            backpointer[(sentence[i])['form']][tag] = max(viterbi[(sentence[i-1])['form']][tagLoop] + tagDistribution[tag][tagLoop] for tagLoop in tagDistribution.keys())\n",
    "                \n",
    "    # termitation step\n",
    "\n",
    "    return viterbi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HMMPredict(test_senteces, tagDistribution, wordDistribution):\n",
    "    b_value = 1\n",
    "    solution = {}\n",
    "    backpointer = {}\n",
    "    \n",
    "    for sentence in test_senteces:\n",
    "        solution = viterbiAlgorithm(sentence, tagDistribution, wordDistribution, b_value)\n",
    "        # recontruct and add to the solution...\n",
    "\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_train_sentences, it_test_sentences, it_val_sentences = load(\"it\")\n",
    "#en_train_sentences, en_test_sentences, en_val_sentences = load(\"en\")\n",
    "#es_train_sentences, es_test_sentences, es_val_sentences = load(\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagDistribution, wordDistribution = HMMFit(it_train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "TokenList<Paolo, ama, Francesca>\n",
      "ama\n",
      "-3.58351893845611\n",
      "-2.8903717578961645\n",
      "-23.7189981105004\n",
      "-3.58351893845611\n",
      "Francesca\n",
      "-7.16703787691222\n",
      "-6.473890696352275\n",
      "-5.375278407684165\n",
      "-7.16703787691222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Paolo': {'N': -1.791759469228055,\n",
       "  'V': -23.025850929940457,\n",
       "  'A': -23.025850929940457,\n",
       "  'AGG': -23.025850929940457},\n",
       " 'ama': {'N': -5.375278407684165,\n",
       "  'V': -4.68213122712422,\n",
       "  'A': -24.412145291060344,\n",
       "  'AGG': -5.375278407684165},\n",
       " 'Francesca': {'N': -8.958797346140274,\n",
       "  'V': -8.265650165580329,\n",
       "  'A': -28.40112933762462,\n",
       "  'AGG': -8.958797346140274}}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HMMPredict(it_test_sentences, tagDistribution, wordDistribution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
