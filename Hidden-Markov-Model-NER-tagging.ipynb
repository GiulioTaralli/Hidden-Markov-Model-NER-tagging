{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import conllu as cl\n",
    "from os.path import join\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructSentences(sentences):\n",
    "    modified_sentences = []\n",
    "    for sentence in sentences:\n",
    "        modified_sentence = []\n",
    "        # create a start token\n",
    "        new_token_start = {\n",
    "            \"id\": len(sentence),\n",
    "            \"form\": \"<s>\",\n",
    "            \"lemma\": \"START\",\n",
    "        }\n",
    "        # create a end token\n",
    "        new_token_end = {\n",
    "            \"id\": len(sentence) + 1,\n",
    "            \"form\": \"</s>\",\n",
    "            \"lemma\": \"END\",\n",
    "        }\n",
    "        # append the start token\n",
    "        modified_sentence.append(new_token_start)\n",
    "        # append the rest of the sentence\n",
    "        for word in sentence:\n",
    "            modified_sentence.append(word)\n",
    "        # append the end token\n",
    "        modified_sentence.append(new_token_end)\n",
    "\n",
    "        # append the new sentence to the list of sentences\n",
    "        modified_sentences.append(modified_sentence)\n",
    "\n",
    "    return modified_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(language):\n",
    "    filePath = \".\\Datasets\\\\\" + language + \"\\\\\"\n",
    "    \n",
    "    with open(join(filePath, \"corpus.conllu\"), 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    train_sentences = reconstructSentences(cl.parse(data))\n",
    "\n",
    "    with open(join(filePath, \"testCorpus.conllu\"), 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    test_sentences = cl.parse(data)\n",
    "\n",
    "    with open(join(filePath, \"val.conllu\"), 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    val_sentences = cl.parse(data)\n",
    "\n",
    "    return train_sentences, test_sentences, val_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the HMM model with the train sentences, it generates 2 outputps containing the probabilities that will be used \n",
    "# in the decoding phase.\n",
    "def HMMFit(train_sentences):\n",
    "    # dictionary of dictionaries\n",
    "    countTag = {}   # Count(tag(i))\n",
    "    transitionCount = {}    # Count(tag(i-1), tag(i))\n",
    "    wordTagCount = {}   # Count(tag(i), word(i))\n",
    "    tagDistribution = {}    # Count(tag(i-1), tag(i)) / Count(tag(i-1))\n",
    "    wordDistribution = {}   # Count(tag(i), word(i))\n",
    "\n",
    "    # complete creation of counTag dictorary\n",
    "    # partial initialization of transitionCount dictionary\n",
    "    # partial creation of wordTagCount dictionary\n",
    "    for sentence in train_sentences:\n",
    "        for word in sentence:\n",
    "            if word['lemma'] in countTag:\n",
    "                countTag[word['lemma']] += 1\n",
    "            else:\n",
    "                countTag[word['lemma']] = 1\n",
    "                transitionCount[word['lemma']] = {}\n",
    "            \n",
    "            if word['form'] not in wordTagCount:\n",
    "                wordTagCount[word['form']] = {}\n",
    "            if word['lemma'] in wordTagCount[word['form']]:\n",
    "                wordTagCount[word['form']][word['lemma']] += 1\n",
    "            else:\n",
    "                wordTagCount[word['form']][word['lemma']] = 1\n",
    "    \n",
    "    # complete inizialization of transitionCount dictionary\n",
    "    for key in transitionCount.keys():\n",
    "        for tag in countTag.keys():\n",
    "            transitionCount[key][tag] = 0\n",
    "\n",
    "    # complete creation of wordTagCount dictionary\n",
    "    for word in wordTagCount.keys():\n",
    "        for tag in countTag.keys():\n",
    "            if tag not in wordTagCount[word]:\n",
    "                wordTagCount[word][tag] = 0\n",
    "\n",
    "    # complete creation of transitionCount dictionary\n",
    "    # previousTag --> Markov's hypothesis of first grade\n",
    "    for tag in transitionCount.keys():\n",
    "        for sentence in train_sentences:\n",
    "            previousTag = None\n",
    "            for word in sentence:\n",
    "                if previousTag == None:\n",
    "                    previousTag = word['lemma']\n",
    "                else:\n",
    "                    if word['lemma'] == tag:\n",
    "                        if previousTag in transitionCount[tag]:\n",
    "                            transitionCount[tag][previousTag] += 1\n",
    "                    previousTag = word['lemma']\n",
    "    \n",
    "    # complete creation of tagDistribution dictionary\n",
    "    # we use the log-probability for better accuracy (since the probability are very small)\n",
    "    # if the probability is 0, then the log-probability will be log(10**-10) \n",
    "    epsilon = 10**-10\n",
    "    for eventTag in countTag.keys():\n",
    "        tagDistribution[eventTag] = {}\n",
    "        for condTag in countTag.keys():\n",
    "            if transitionCount[eventTag][condTag]/countTag[condTag] == 0:\n",
    "                tagDistribution[eventTag][condTag] = math.log(epsilon)\n",
    "            else:\n",
    "                tagDistribution[eventTag][condTag] = math.log(transitionCount[eventTag][condTag]/countTag[condTag])\n",
    "            \n",
    "    # complete creation of wordDistribution dictionary\n",
    "    # we use the log-probability for better accuracy (since the probability are very small)\n",
    "    # if the probability is 0, then the log-probability will be log(10**-10)\n",
    "    for word in wordTagCount.keys():\n",
    "        wordDistribution[word] = {}\n",
    "        for tag in countTag.keys():\n",
    "            if wordTagCount[word][tag]/countTag[tag] == 0:\n",
    "                wordDistribution[word][tag] = math.log(epsilon)\n",
    "            else:\n",
    "                wordDistribution[word][tag] = math.log(wordTagCount[word][tag]/countTag[tag])\n",
    "        \n",
    "    return tagDistribution, wordDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbiAlgorithm(sentence, tagDistribution, wordDistribution, b_value):\n",
    "    epsilon = 10**-10\n",
    "    columns = len(sentence) # words\n",
    "    rows = len(tagDistribution.keys())-2 #tags\n",
    "    viterbi = [[math.log(epsilon) for _ in range(columns)] for _ in range(rows)]\n",
    "    backpointer = [[\"\" for _ in range(columns)] for _ in range(rows)]\n",
    "\n",
    "    # initialization step\n",
    "    i = 0\n",
    "    for tag in tagDistribution.keys():\n",
    "        if tag != \"START\" and tag != \"END\": \n",
    "            viterbi[i][0] = tagDistribution[tag][\"START\"] + b_value\n",
    "            backpointer[i][0] = \"0\"\n",
    "            i += 1\n",
    "\n",
    "    tagList = list(tagDistribution)\n",
    "    tagList.remove(\"START\")\n",
    "    tagList.remove(\"END\")\n",
    "\n",
    "    # recursion step\n",
    "    for i in range(1, len(sentence)):\n",
    "        j = 0\n",
    "        for tag in tagDistribution.keys():\n",
    "            if tag != \"START\" and tag != \"END\":\n",
    "                viterbi[j][i] = max(viterbi[tagLoop][i-1] + wordDistribution[(sentence[i])['form']][tag] + tagDistribution[tag][tagList[tagLoop]] for tagLoop in range(len(tagList)))\n",
    "                _, maxTag = max((viterbi[tagLoop][i-1] + tagDistribution[tag][tagList[tagLoop]], tagLoop) for tagLoop in range(len(tagList)))\n",
    "                backpointer[j][i] = maxTag\n",
    "                j += 1\n",
    "\n",
    "    # termitation step\n",
    "    viterbi_end = max(viterbi[tagLoop][len(sentence)-1] + tagDistribution[\"END\"][tagList[tagLoop]] for tagLoop in range(len(tagList)))\n",
    "    _, maxTag = max((viterbi[tagLoop][len(sentence)-1] + tagDistribution[\"END\"][tagList[tagLoop]], tagLoop) for tagLoop in range(len(tagList)))\n",
    "    backpointer_end = maxTag\n",
    "    \n",
    "    # reconstructing the solution\n",
    "    path = [None] * len(sentence)\n",
    "    solution = [None] * len(sentence)\n",
    "    path[len(sentence)-1] = backpointer_end\n",
    "    solution[len(sentence)-1] = tagList[backpointer_end]\n",
    "    for i in range(len(sentence)-2, -1, -1): # decrementing loop, 0 inclusive\n",
    "        path[i] = backpointer[path[i+1]][i+1]\n",
    "        solution[i] = tagList[path[i]]\n",
    "\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(predict, test_sentences):\n",
    "    predictions = 0\n",
    "    correctPredictions = 0\n",
    "\n",
    "    # accuracy\n",
    "    i = 0\n",
    "    for sentence in test_sentences:\n",
    "        j = 0\n",
    "        for word in sentence:\n",
    "            if word['lemma'] == predict[i][j]:\n",
    "                correctPredictions += 1\n",
    "            predictions += 1\n",
    "            j += 1\n",
    "        i += 1\n",
    "    accuracy = correctPredictions / predictions\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HMMPredict(test_senteces, tagDistribution, wordDistribution):\n",
    "    b_value = 0 # in log probabilities 0 is the neutral element\n",
    "    solution = []\n",
    "\n",
    "    for sentence in test_senteces:\n",
    "        sentenceSolution = viterbiAlgorithm(sentence, tagDistribution, wordDistribution, b_value)\n",
    "        # recontruct and add to the solution...\n",
    "        solution.append(sentenceSolution)\n",
    "\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_train_sentences, it_test_sentences, it_val_sentences = load(\"it\")\n",
    "#en_train_sentences, en_test_sentences, en_val_sentences = load(\"en\")\n",
    "#es_train_sentences, es_test_sentences, es_val_sentences = load(\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagDistribution, wordDistribution = HMMFit(it_train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', 2, 1], ['0', 0, 1], ['0', 0, 1], ['0', 0, 1]]\n",
      "[['0', 2, 1, 2], ['0', 0, 0, 2], ['0', 0, 1, 2], ['0', 0, 0, 2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['N', 'V', 'N'], ['N', 'V', 'A', 'N']]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = HMMPredict(it_test_sentences, tagDistribution, wordDistribution)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(predict, it_test_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
