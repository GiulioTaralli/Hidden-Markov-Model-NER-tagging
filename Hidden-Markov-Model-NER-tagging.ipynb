{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import conllu as cl\n",
    "from os.path import join\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructSentences(sentences):\n",
    "    modified_sentences = []\n",
    "    for sentence in sentences:\n",
    "        modified_sentence = []\n",
    "        # create a start token\n",
    "        new_token_start = {\n",
    "            \"id\": len(sentence),\n",
    "            \"form\": \"<s>\",\n",
    "            \"lemma\": \"START\",\n",
    "        }\n",
    "        # create a end token\n",
    "        new_token_end = {\n",
    "            \"id\": len(sentence) + 1,\n",
    "            \"form\": \"</s>\",\n",
    "            \"lemma\": \"END\",\n",
    "        }\n",
    "        # append the start token\n",
    "        modified_sentence.append(new_token_start)\n",
    "        # append the rest of the sentence\n",
    "        for word in sentence:\n",
    "            modified_sentence.append(word)\n",
    "        # append the end token\n",
    "        modified_sentence.append(new_token_end)\n",
    "\n",
    "        # append the new sentence to the list of sentences\n",
    "        modified_sentences.append(modified_sentence)\n",
    "\n",
    "    return modified_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(language):\n",
    "    filePath = \".\\Datasets\\\\\" + language + \"\\\\\"\n",
    "    \n",
    "    with open(join(filePath, \"corpus.conllu\"), 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    train_sentences = reconstructSentences(cl.parse(data))\n",
    "\n",
    "    with open(join(filePath, \"testCorpus.conllu\"), 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    test_sentences = cl.parse(data)\n",
    "\n",
    "    with open(join(filePath, \"val.conllu\"), 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    val_sentences = cl.parse(data)\n",
    "\n",
    "    return train_sentences, test_sentences, val_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the HMM model with the train sentences, it generates 2 outputps containing the probabilities that will be used \n",
    "# in the decoding phase.\n",
    "def HMMFit(train_sentences):\n",
    "    # dictionary of dictionaries\n",
    "    countTag = {}   # Count(tag(i))\n",
    "    transitionCount = {}    # Count(tag(i-1), tag(i))\n",
    "    wordTagCount = {}   # Count(tag(i), word(i))\n",
    "    tagDistribution = {}    # Count(tag(i-1), tag(i)) / Count(tag(i-1))\n",
    "    wordDistribution = {}   # Count(tag(i), word(i))\n",
    "\n",
    "    # complete creation of counTag dictorary\n",
    "    # partial initialization of transitionCount dictionary\n",
    "    # partial creation of wordTagCount dictionary\n",
    "    for sentence in train_sentences:\n",
    "        for word in sentence:\n",
    "            if word['lemma'] in countTag:\n",
    "                countTag[word['lemma']] += 1\n",
    "            else:\n",
    "                countTag[word['lemma']] = 1\n",
    "                transitionCount[word['lemma']] = {}\n",
    "            \n",
    "            if word['form'] not in wordTagCount:\n",
    "                wordTagCount[word['form']] = {}\n",
    "            if word['lemma'] in wordTagCount[word['form']]:\n",
    "                wordTagCount[word['form']][word['lemma']] += 1\n",
    "            else:\n",
    "                wordTagCount[word['form']][word['lemma']] = 1\n",
    "    \n",
    "    # complete inizialization of transitionCount dictionary\n",
    "    for key in transitionCount.keys():\n",
    "        for tag in countTag.keys():\n",
    "            transitionCount[key][tag] = 0\n",
    "\n",
    "    # complete creation of wordTagCount dictionary\n",
    "    for word in wordTagCount.keys():\n",
    "        for tag in countTag.keys():\n",
    "            if tag not in wordTagCount[word]:\n",
    "                wordTagCount[word][tag] = 0\n",
    "\n",
    "    # complete creation of transitionCount dictionary\n",
    "    # previousTag --> Markov's hypothesis of first grade\n",
    "    for tag in transitionCount.keys():\n",
    "        for sentence in train_sentences:\n",
    "            previousTag = None\n",
    "            for word in sentence:\n",
    "                if previousTag == None:\n",
    "                    previousTag = word['lemma']\n",
    "                else:\n",
    "                    if word['lemma'] == tag:\n",
    "                        if previousTag in transitionCount[tag]:\n",
    "                            transitionCount[tag][previousTag] += 1\n",
    "                    previousTag = word['lemma']\n",
    "    \n",
    "    # complete creation of tagDistribution dictionary\n",
    "    # we use the log-probability for better accuracy (since the probability are very small)\n",
    "    # if the probability is 0, then the log-probability will be log(10**-10) \n",
    "    epsilon = 10**-10\n",
    "    for eventTag in countTag.keys():\n",
    "        tagDistribution[eventTag] = {}\n",
    "        for condTag in countTag.keys():\n",
    "            if transitionCount[eventTag][condTag]/countTag[condTag] == 0:\n",
    "                tagDistribution[eventTag][condTag] = math.log(epsilon)\n",
    "            else:\n",
    "                tagDistribution[eventTag][condTag] = math.log(transitionCount[eventTag][condTag]/countTag[condTag])\n",
    "            \n",
    "    # complete creation of wordDistribution dictionary\n",
    "    # we use the log-probability for better accuracy (since the probability are very small)\n",
    "    # if the probability is 0, then the log-probability will be log(10**-10)\n",
    "    for word in wordTagCount.keys():\n",
    "        wordDistribution[word] = {}\n",
    "        for tag in countTag.keys():\n",
    "            if wordTagCount[word][tag]/countTag[tag] == 0:\n",
    "                wordDistribution[word][tag] = math.log(epsilon)\n",
    "            else:\n",
    "                wordDistribution[word][tag] = math.log(wordTagCount[word][tag]/countTag[tag])\n",
    "        \n",
    "    return tagDistribution, wordDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbiAlgorithm(sentence, tagDistribution, wordDistribution, b_value):\n",
    "    viterbi = {}\n",
    "    backpointer = {}\n",
    "\n",
    "    # todo: add the descrpt.\n",
    "    for word in sentence:\n",
    "        viterbi[word['form']] = {}\n",
    "        backpointer[word['form']] = {}\n",
    "    viterbi[\"END\"] = {}\n",
    "    backpointer[\"END\"] = {}\n",
    "\n",
    "    # initialization step\n",
    "    for tag in tagDistribution.keys():\n",
    "        viterbi[(sentence[0])['form']][tag] = tagDistribution[tag][\"START\"] + b_value\n",
    "        print(\"!!!!!!!initialization: \" + str(tagDistribution[tag][\"START\"]))\n",
    "        backpointer[(sentence[0])['form']][tag] = 0\n",
    "\n",
    "    # recursion step\n",
    "    print(len(sentence))\n",
    "    print(sentence)\n",
    "    for i in range(1, len(sentence)):\n",
    "        print((sentence[i])['form'])\n",
    "        for tag in tagDistribution.keys():\n",
    "            if tag != \"START\" and tag != \"END\":\n",
    "                viterbi[(sentence[i])['form']][tag] = max(viterbi[(sentence[i-1])['form']][tagLoop] + wordDistribution[(sentence[i])['form']][tag] + tagDistribution[tag][tagLoop] for tagLoop in tagDistribution.keys() if tagLoop not in [\"START\", \"END\"])\n",
    "                #backpointer[(sentence[i])['form']][tag] = max(viterbi[(sentence[i-1])['form']][tagLoop] + tagDistribution[tag][tagLoop] for tagLoop in tagDistribution.keys())\n",
    "                maxVit, maxTag = max((viterbi[(sentence[i-1])['form']][tagLoop] + tagDistribution[tag][tagLoop], tagLoop) for tagLoop in tagDistribution.keys() if tagLoop not in [\"START\", \"END\"])\n",
    "                print(\"PAROLA CORRENTE: \" + sentence[i]['form'] + \" MAXVIT: \" + str(maxVit) + \" currentTAG: \" + tag + \" maxtag: \" + maxTag)\n",
    "                backpointer[(sentence[i])['form']][tag] = maxTag\n",
    "\n",
    "    # termitation step\n",
    "    viterbi[\"END\"] = max(viterbi[(sentence[len(sentence)-1])['form']][tagLoop] + tagDistribution[\"END\"][tagLoop] for tagLoop in tagDistribution.keys() if tagLoop not in [\"START\", \"END\"])\n",
    "    #backpointer[\"END\"][tag] = max(viterbi[(sentence[len(sentence)])['form']][tagLoop] + tagDistribution[\"END\"][tagLoop] for tagLoop in tagDistribution.keys())\n",
    "    maxVit, maxTag = max((viterbi[(sentence[len(sentence)-1])['form']][tagLoop] + tagDistribution[\"END\"][tagLoop], tagLoop) for tagLoop in tagDistribution.keys() if tagLoop not in [\"START\", \"END\"])\n",
    "    backpointer[\"END\"] = maxTag\n",
    "\n",
    "    return viterbi, backpointer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HMMPredict(test_senteces, tagDistribution, wordDistribution):\n",
    "    b_value = 0 # in log probabilities 0 is the neutral element\n",
    "    solution = {}\n",
    "    backpointer = {}\n",
    "    \n",
    "    for sentence in test_senteces:\n",
    "        solution, backpointer = viterbiAlgorithm(sentence, tagDistribution, wordDistribution, b_value)\n",
    "        # recontruct and add to the solution...\n",
    "\n",
    "    return solution, backpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'id': 2, 'form': '<s>', 'lemma': 'START'},\n",
       "  {'id': 0, 'form': 'Paolo', 'lemma': 'N'},\n",
       "  {'id': 1, 'form': 'pesca', 'lemma': 'V'},\n",
       "  {'id': 3, 'form': '</s>', 'lemma': 'END'}],\n",
       " [{'id': 4, 'form': '<s>', 'lemma': 'START'},\n",
       "  {'id': 0, 'form': 'Giovanni', 'lemma': 'N'},\n",
       "  {'id': 1, 'form': 'ama', 'lemma': 'V'},\n",
       "  {'id': 2, 'form': 'i', 'lemma': 'A'},\n",
       "  {'id': 3, 'form': 'cani', 'lemma': 'N'},\n",
       "  {'id': 5, 'form': '</s>', 'lemma': 'END'}],\n",
       " [{'id': 2, 'form': '<s>', 'lemma': 'START'},\n",
       "  {'id': 0, 'form': 'Francesca', 'lemma': 'N'},\n",
       "  {'id': 1, 'form': 'ama', 'lemma': 'N'},\n",
       "  {'id': 3, 'form': '</s>', 'lemma': 'END'}],\n",
       " [{'id': 3, 'form': '<s>', 'lemma': 'START'},\n",
       "  {'id': 0, 'form': 'Una', 'lemma': 'A'},\n",
       "  {'id': 1, 'form': 'pesca', 'lemma': 'N'},\n",
       "  {'id': 2, 'form': 'Francesca', 'lemma': 'AGG'},\n",
       "  {'id': 4, 'form': '</s>', 'lemma': 'END'}]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it_train_sentences, it_test_sentences, it_val_sentences = load(\"it\")\n",
    "#en_train_sentences, en_test_sentences, en_val_sentences = load(\"en\")\n",
    "#es_train_sentences, es_test_sentences, es_val_sentences = load(\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'START': {'START': -23.025850929940457,\n",
       "  'N': -23.025850929940457,\n",
       "  'V': -23.025850929940457,\n",
       "  'END': -23.025850929940457,\n",
       "  'A': -23.025850929940457,\n",
       "  'AGG': -23.025850929940457},\n",
       " 'N': {'START': -0.2876820724517809,\n",
       "  'N': -1.791759469228055,\n",
       "  'V': -23.025850929940457,\n",
       "  'END': -23.025850929940457,\n",
       "  'A': 0.0,\n",
       "  'AGG': -23.025850929940457},\n",
       " 'V': {'START': -23.025850929940457,\n",
       "  'N': -1.0986122886681098,\n",
       "  'V': -23.025850929940457,\n",
       "  'END': -23.025850929940457,\n",
       "  'A': -23.025850929940457,\n",
       "  'AGG': -23.025850929940457},\n",
       " 'END': {'START': -23.025850929940457,\n",
       "  'N': -1.0986122886681098,\n",
       "  'V': -0.6931471805599453,\n",
       "  'END': -23.025850929940457,\n",
       "  'A': -23.025850929940457,\n",
       "  'AGG': 0.0},\n",
       " 'A': {'START': -1.3862943611198906,\n",
       "  'N': -23.025850929940457,\n",
       "  'V': -0.6931471805599453,\n",
       "  'END': -23.025850929940457,\n",
       "  'A': -23.025850929940457,\n",
       "  'AGG': -23.025850929940457},\n",
       " 'AGG': {'START': -23.025850929940457,\n",
       "  'N': -1.791759469228055,\n",
       "  'V': -23.025850929940457,\n",
       "  'END': -23.025850929940457,\n",
       "  'A': -23.025850929940457,\n",
       "  'AGG': -23.025850929940457}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagDistribution, wordDistribution = HMMFit(it_train_sentences)\n",
    "tagDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!initialization: -23.025850929940457\n",
      "!!!!!!!initialization: -0.2876820724517809\n",
      "!!!!!!!initialization: -23.025850929940457\n",
      "!!!!!!!initialization: -23.025850929940457\n",
      "!!!!!!!initialization: -1.3862943611198906\n",
      "!!!!!!!initialization: -23.025850929940457\n",
      "3\n",
      "TokenList<Paolo, ama, Francesca>\n",
      "ama\n",
      "PAROLA CORRENTE: ama MAXVIT: -1.3862943611198906 currentTAG: N maxtag: A\n",
      "PAROLA CORRENTE: ama MAXVIT: -1.3862943611198908 currentTAG: V maxtag: N\n",
      "PAROLA CORRENTE: ama MAXVIT: -23.313533002392237 currentTAG: A maxtag: N\n",
      "PAROLA CORRENTE: ama MAXVIT: -2.0794415416798357 currentTAG: AGG maxtag: N\n",
      "Francesca\n",
      "PAROLA CORRENTE: Francesca MAXVIT: -4.969813299576 currentTAG: N maxtag: N\n",
      "PAROLA CORRENTE: Francesca MAXVIT: -4.276666119016055 currentTAG: V maxtag: N\n",
      "PAROLA CORRENTE: Francesca MAXVIT: -2.7725887222397816 currentTAG: A maxtag: V\n",
      "PAROLA CORRENTE: Francesca MAXVIT: -4.969813299576 currentTAG: AGG maxtag: N\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'Paolo': {'START': -23.025850929940457,\n",
       "   'N': -0.2876820724517809,\n",
       "   'V': -23.025850929940457,\n",
       "   'END': -23.025850929940457,\n",
       "   'A': -1.3862943611198906,\n",
       "   'AGG': -23.025850929940457},\n",
       "  'ama': {'N': -3.1780538303479453,\n",
       "   'V': -2.079441541679836,\n",
       "   'A': -46.339383932332694,\n",
       "   'AGG': -25.10529247162029},\n",
       "  'Francesca': {'N': -6.761572768804054,\n",
       "   'V': -27.302517048956513,\n",
       "   'A': -25.79843965218024,\n",
       "   'AGG': -4.969813299576},\n",
       "  'END': -4.969813299576},\n",
       " {'Paolo': {'START': 0, 'N': 0, 'V': 0, 'END': 0, 'A': 0, 'AGG': 0},\n",
       "  'ama': {'N': 'A', 'V': 'N', 'A': 'N', 'AGG': 'N'},\n",
       "  'Francesca': {'N': 'N', 'V': 'N', 'A': 'V', 'AGG': 'N'},\n",
       "  'END': 'AGG'})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HMMPredict(it_test_sentences, tagDistribution, wordDistribution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
